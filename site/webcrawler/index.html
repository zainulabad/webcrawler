



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
        <meta name="description" content="A Material Design theme for MkDocs">
      
      
        <link rel="canonical" href="https://squidfunk.github.io/mkdocs-material/webcrawler/">
      
      
        <meta name="author" content="Martin Donath">
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.2.0">
    
    
      
        <title>Web Crawler - Penambangan Web - Web Crawler</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/application.750b69bd.css">
      
        <link rel="stylesheet" href="../assets/stylesheets/application-palette.224b79ff.css">
      
      
        
        
        <meta name="theme-color" content="#3f51b5">
      
    
    
      <script src="../assets/javascripts/modernizr.74668098.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../assets/fonts/material-icons.css">
    
    
    
      
        
<script>
  window.ga = window.ga || function() {
    (ga.q = ga.q || []).push(arguments)
  }
  ga.l = +new Date
  /* Setup integration and send page view */
  ga("create", "None", "auto")
  ga("set", "anonymizeIp", true)
  ga("send", "pageview")
  /* Register handler to log search on blur */
  document.addEventListener("DOMContentLoaded", () => {
    if (document.forms.search) {
      var query = document.forms.search.query
      query.addEventListener("blur", function() {
        if (this.value) {
          var path = document.location.pathname;
          ga("send", "pageview", path + "?q=" + this.value)
        }
      })
    }
  })
</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
      
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448"
    viewBox="0 0 416 448" id="__github">
  <path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19-18.125
        8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5
        18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75
        19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19
        18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360
        304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25
        2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8
        38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0
        37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25
        82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75
        1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75
        0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25
        30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5
        46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34
        99.5z" />
</svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#web-crawler" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://squidfunk.github.io/mkdocs-material/" title="Penambangan Web - Web Crawler" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              Penambangan Web - Web Crawler
            </span>
            <span class="md-header-nav__topic">
              Web Crawler
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  

<a href="https://github.com/squidfunk/mkdocs-material" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    squidfunk/mkdocs-material
  </div>
</a>
          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
        

<nav class="md-tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
      
    </ul>
  </div>
</nav>
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="https://squidfunk.github.io/mkdocs-material/" title="Penambangan Web - Web Crawler" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    Penambangan Web - Web Crawler
  </label>
  
    <div class="md-nav__source">
      


  

<a href="https://github.com/squidfunk/mkdocs-material" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    squidfunk/mkdocs-material
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Web Crawler
      </label>
    
    <a href="./" title="Web Crawler" class="md-nav__link md-nav__link--active">
      Web Crawler
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#apa-itu-web-crawler" title="APA ITU WEB CRAWLER ?" class="md-nav__link">
    APA ITU WEB CRAWLER ?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#text-processing" title="TEXT PROCESSING" class="md-nav__link">
    TEXT PROCESSING
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vector-space-model-vsm" title="VECTOR SPACE MODEL (VSM)" class="md-nav__link">
    VECTOR SPACE MODEL (VSM)
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#apa-itu-web-crawler" title="APA ITU WEB CRAWLER ?" class="md-nav__link">
    APA ITU WEB CRAWLER ?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#text-processing" title="TEXT PROCESSING" class="md-nav__link">
    TEXT PROCESSING
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vector-space-model-vsm" title="VECTOR SPACE MODEL (VSM)" class="md-nav__link">
    VECTOR SPACE MODEL (VSM)
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                <h1 id="web-crawler">WEB CRAWLER<a class="headerlink" href="#web-crawler" title="Permanent link">&para;</a></h1>
<p>==================================================</p>
<p>Nama : Zainul Abad</p>
<p>NIM : 160411100116</p>
<p>Mata Kuliah : Penambangan WEB.</p>
<p>==================================================</p>
<h3 id="apa-itu-web-crawler">APA ITU WEB CRAWLER ?<a class="headerlink" href="#apa-itu-web-crawler" title="Permanent link">&para;</a></h3>
<p><strong>Web Crawler</strong> adalah sebuah program dengan metode tertentu yang berfungsi untuk melakukan Scan atau crawl ke semua halaman internet untuk mencari data yang diinginkan.</p>
<p>Pada WEB Crawl kita membutuhkan Link Web.</p>
<p>pada web crawl kali ini saya menggunakan Jupyter  (Python versi 3) , dan pada Python ini saya menggunakan library :</p>
<ol>
<li>
<p><strong>BeautifulSoup4</strong></p>
</li>
<li>
<p><strong>Requesests</strong></p>
</li>
<li>
<p><strong>csv</strong></p>
</li>
<li>
<p><strong>SQLite</strong></p>
</li>
<li>
<p><strong>numpy</strong></p>
</li>
<li>
<p><strong>scipy</strong></p>
</li>
<li>
<p><strong>scikit-learn</strong></p>
</li>
<li>
<p><strong>scikit-fuzzy</strong></p>
</li>
</ol>
<p>=&gt; Jadi pada Tahap Pertama Kita mencari link web yang akan kita crawl.</p>
<p>Link : <a href="">https://www.suara.com/bola/bola-category/bola-indonesia</a></p>
<p>kita request link:</p>
<p><code>req = Request("https://www.suara.com/bola/bola-category/bola-indonesia")
html_page = urlopen(req)
soup = BeautifulSoup(html_page, "lxml")</code></p>
<p>=&gt; Langkah selanjutnya kita mencari tahu tag pada artikel tersebut, kita perlu inspect elemen.</p>
<p>tag pada artikel saya . Tag judul "h4" dengan class="past-title"</p>
<p><strong>source code</strong> dari program saya :</p>
<p><code>html=urlopen("https://www.suara.com/bola/bola-category/bola-indonesia").read()
soup=BeautifulSoup(html,"lxml")
ar= soup.find_all("li","item-outer")
i=1
judul=[]
for j in ar:
    dapat_judul=j.find('h4','post-title').get_text().replace("\n","")
    judul.append(dapat_judul)
print(judul)</code></p>
<p>=&gt; <strong>Code</strong> diatas digunakan untuk mendapatkan judul dan deskripsi pada artikel yang kita crawling.</p>
<p>==========================</p>
<p>=&gt; kemudian kita gunakan code dibawah untuk mengambil link. dan linknya berada dalam tag 'a' dengan class 'ellipsis2'</p>
<p><code>links = []
deskripsifull=[]
for link in soup.findAll('a','ellipsis2'):
    links.append(link.get('href')) #menyimpan link
print(links)</code></p>
<p>==========================</p>
<p>=&gt; <strong>Source Code</strong> dibawah ini digunakan untuk jika ada kesalahan saat crawl. jika ada data yang sama data tersebut tidak dimasukkan di array.</p>
<p>`des=[]
for i in links: 
    deslink=urlopen(i).read() #membuka link 1per1
    soup1=BeautifulSoup(deslink,"lxml")
    ketdes= soup1.find_all("article")
    da=[]
    for j in ketdes:
        desk=""
        konten= soup1.find_all('article','content-article')
        for i in soup1.find('article','content-article').find_all('p'):
            desk=desk+i.text
        if(not desk in des):
            des.append(desk)</p>
<p>print(len(des))`</p>
<p>==========================</p>
<p>=&gt; <strong>Source code</strong> dibawah ini digunakan untuk membuang karakter dan space yang tidak dibutuhkan.</p>
<p><code>for j in des:
    print(j.replace('"',' ').replace('.',' ').replace('/',' ').replace(',',' ').replace('""',' '))
    print("==========================================")</code></p>
<p>==========================</p>
<p><code>import pandas as pd</code>
<code>import numpy as np</code> 
<code>artikel = {'judul':judul,'deskripsi':des}</code>
<code>df=pd.DataFrame(artikel,columns=['judul','deskripsi'])</code></p>
<p><code>df.to_csv("dataku.csv",sep=',')</code>
<code>df.sort_values('judul',ascending=True)</code></p>
<p><code>per_kata=[]</code>
<code>for kt in des:</code>
    <code>per_kata.append(kt.split())</code></p>
<p><code>print(per_kata)</code>
<code>print(len(des))</code></p>
<p>==========</p>
<p>`import pandas as pd<br />
data = pd.read_csv("dataku.csv") 
deskr=[]
for i in data['deskripsi']:
    deskr.append(i)</p>
<p>print(deskr)`</p>
<p>=&gt; <strong>Source Code</strong> diatas digunakan untuk import judul dan deskripsi ke data CSV.</p>
<p>==========================</p>
<h3 id="text-processing">TEXT PROCESSING<a class="headerlink" href="#text-processing" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Tokenisasi</strong> - Tokenisasi adalah proses untuk membagi teks yang dapat berupa kalimat, paragraf atau dokumen, menjadi token-token. Sebagai contoh, tokenisasi dari kalimat " Pelajar Indonesia di China Ditemukan Tewas. " menghasilkan enam token, yakni: " Pelajar ", " Indonesia ", " di ", " China ", " Ditemukan ", " Tewas ". Biasanya, yang menjadi acuan pemisah antar token adalah spasi dan tanda baca. Tokenisasi berguna untuk analisis teks lebih lanjut dan dipakai dalam ilmu linguistik. </li>
<li><strong>Filtering</strong> - Tahap Filtering adalah tahap mengambil kata-kata penting dari hasil token. Bisa menggunakan algoritma stoplist (membuang kata kurang penting) atau wordlist (menyimpan kata penting). Stoplist/stopword adalah kata-kata yang tidak deskriptif yang dapat dibuang dalam pendekatan bag-of-words. Contoh stopwords adalah “yang”, “dan”, “di”, “dari” dan seterusnya</li>
<li><strong>Stemming</strong> - Stemming adalah proses pemetaan dan penguraian bentuk dari suatu kata menjadi bentuk kata dasarnya. Gampangnya, proses mengubah kata berimbuhan menjadi kata dasar
   Stemming (atau mungkin lebih tepatnya lemmatization?) adalah proses mengubah kata berimbuhan menjadi kata dasar. Aturan-aturan bahasa diterapkan untuk menanggalkan imbuhan-imbuhan itu.</li>
</ol>
<p><strong>Contohnya :</strong></p>
<ol>
<li>Bermain =&gt; Main</li>
<li>Berolahraga =&gt; Olahraga</li>
</ol>
<p>==========================</p>
<p>=&gt; <strong>Source Code</strong> library dibawah ini digunakan untuk import csv from sastrawi. library ini digunakan untuk mengambil kata dasar pada artikel yang kita crawling.</p>
<p><code>`import csv</code>
<code>from Sastrawi.Stemmer.StemmerFactory import StemmerFactory</code>
<code>from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory</code></p>
<p><code>SWfactory = StopWordRemoverFactory()</code>
<code>factory = StemmerFactory()</code>
<code>stemmer = factory.create_stemmer()</code>
<code>stopword = SWfactory.create_stop_word_remover()</code>
<code>Sfactory = StemmerFactory()</code></p>
<p>==========================</p>
<p>=&gt; <strong>Source Code</strong> dibawah ini digunakan untuk memisah kata dasar pada sebuah artikel yang kita crawling.</p>
<p><code>`f = open('dataku.csv', 'r')</code>
<code>reader = csv.reader(f)</code>
<code>deskripsi=[];</code> 
<code>judul=[];</code>   </p>
<p><code>jumlah=0</code>
<code>for row in reader:</code>
    <code>jumlah+=1;</code>
    <code>if jumlah!=1:</code>
        <code>judul.append(str(row[1]))</code>
        <code>deskripsi.append(str(row[2]))</code></p>
<p><code>des_katadasar=[]</code></p>
<p><code>for i in deskripsi:</code>
    <code>hasil = ''</code>
    <code>kata_per_artikel={}</code>
    <code>for j in i.split():</code>
        <code>if j.isalpha():</code>
            <code>stop = stopword.remove(j)</code>
            <code>stem = stemmer.stem(stop)</code>
            <code>hasil += stem+ ' '</code>
    <code>des_katadasar.append(hasil)</code></p>
<p>setelah melalui tahap ini kita akan membuat VSM atau Vector Space Model.</p>
<p>==========================</p>
<h3 id="vector-space-model-vsm">VECTOR SPACE MODEL (VSM)<a class="headerlink" href="#vector-space-model-vsm" title="Permanent link">&para;</a></h3>
<p>Pada VSM sebuah dokumen akan direpresentasikan dalam sebuah vektor, dimana setiap nilai pada vektor tersebut mewakili bobot term yang bersangkutan.
adalah model aljabar yang merepresentasikan kumpulan dokumen sebagai vetctor. VSM dapat diaplikasikan dalam klasifikasi dokumen, clustering dokumen, dan scoring dokumen terhadap sebuah query. Dalam VSM setiap dokumen direpresentasikan sebagai sebuah vector, dimana nilai dari setiap nilai dari vector tersebut mewakili weight sebuah term. Dalam menghitung term-weight dapa digunakan beberapa teknik seperti frequency, binary-document vector, atau tf-idf.</p>
<p><strong>Contoh :</strong></p>
<ul>
<li>Artikel 1 : siswa itu sedang belajar</li>
<li>Artikel 2 : siswa itu sedang bermain bola</li>
</ul>
<p>kita akan menghitung setiap kata dan kita tampung pada table CSV.</p>
<p>​                       Siswa       itu     sedang      belajar     bermain     bola</p>
<p>Artikal1            1               1               1               0               0                   0</p>
<p>Artikel2            1               1               1               0               1                   1</p>
<p><strong>Untuk Codenya :</strong></p>
<p>=&gt; <strong>Source Code</strong> dibawah digunakan untuk menghitung setiap kata.</p>
<p><code>def countWord(txt, ngram=1):
d = dict()
token = tokenisasi(txt, ngram)
for i  in token:
if d.get(i) == None:
d[i] = txt.count(i)
return d</code></p>
<p>=&gt; <strong>Source Code</strong> dibawah digunakan untuk membangun VSM.</p>
<p>`def add_row_VSM(d):
VSM.append([])
for i in VSM[0]:
if d.get(i) == None:
VSM[-1].append(0)
else :
VSM[-1].append(d.pop(i)); 
for i in d:
VSM[0].append(i)
for j in range(1, len(VSM)-1):</p>
<h1 id="vsmjinsert-20">VSM[j].insert(-2,0)<a class="headerlink" href="#vsmjinsert-20" title="Permanent link">&para;</a></h1>
<p>VSM[j].append(0)
VSM[-1].append(d.get(i))`</p>
<p>`cursor = conn.execute("SELECT * from jurnal2")
cursor = cursor.fetchall()
cursor = cursor[:60]
pertama = True
corpus = list()
label = list()
c=1
n = int(input("ngram : "))</p>
<h1 id="n1">n=1<a class="headerlink" href="#n1" title="Permanent link">&para;</a></h1>
<p>for row in cursor:
print ('Proses : %.2f' %((c/len(cursor))*100) + '%'); c+=1
label.append(row[-1])
txt = row[-2]
cleaned = preprosesing(txt)
cleaned = cleaned[:-1]
corpus.append(cleaned)
d = countWord(cleaned, n)
if pertama:
pertama = False
VSM = list((list(), list()))
for key in d:
VSM[0].append(key)
VSM[1].append(d[key])
else:
add_row_VSM(d)`</p>
<p>=&gt; <strong>Source Code</strong>  dibawah untuk menampilkan hasil CSV yang sudah kita create tadi.</p>
<p><code>write_csv("bow_manual_%d.csv"%n, VSM)
feature_name = VSM[0]
bow = np.array(VSM[1:])</code></p>
<p>==========================</p>
<h3 id="tf-idf">TF - IDF<a class="headerlink" href="#tf-idf" title="Permanent link">&para;</a></h3>
<p><strong>TF</strong> - TF atau term frequency adalah weighting scheme yang digunakan untuk menentukan relevansi dokumen dengan sebuah query (term). TF menentukan bobot relevansi sebuah dokumen dan term berdasarkan frekuensi kemunculan term pada dokumen terkait. Untuk menghitung TF terdapat beberapa jenis fungsi yang dapat digunakan</p>
<p><strong>DF</strong> - Jika TF adalah frekuensi kemunculan suatu term pada sebuah dokumen, DF merupakan jumlah dokumen dimana terdapat term yang bersangkutan. Konsep DF sendiri dilatarbelakangin oleh masalah pada TF, dimana semua term dianggap sama penting, sehingga term yang memiliki sedikit atau tidak memiliki discrimination power dapat mempengaruhi akurasi dalam menentukan relevansi antara term dan dokumen. Ide dari DF adalah dengan mengurangi bobot TF suatu term dengan membaginya dengan frekuensi term terhadap koleksi dokumen (DF). Jadi sebuah term yang memiliki bobot TF yang besar namun dengan bobot DF yang besar pula tidak akan memiliki pengaruh yang besar dalam menentukan sebuah relevansi .</p>
<p><strong>IDF</strong> - IDF adalah inverse dari DF, IDF akan melakukan proses scaling pada TF. Term yang memiliki DF yang rendah akan memiliki IDF yang tinggi. Dengan kata lain, sebuah term yang jarang ditemui pada koleksi dokumen atau bisa dikatakan sebagai term khusus akan memiliki nilai IDF yang tinggi. Untuk menghitung IDF pada sebuah term pada sebuah koleksi dokumen dapat menggunakan fungsi dibawah ini,</p>
<p><strong>TF - IDF</strong> -- Selain menggunakan Bag of Words, kita juga bisa menggunakan metode TF-IDF. Hal ini karena Bag of Word memiliki kelemahan tersendiri.
TF-IDF sendiri merupakan kepanjangan dari Term Frequence (frekuensi Kata) dan Invers Document Frequence (invers frekuensi Dokumen). Rumus TF-IDF sendiri terbilang mudah karena hanya TFxIDF.
Kita telah mencari TF sebelumnya (yaitu Bag of Words), karena konsep keduanya yang memang sama. Sekarang kita tinggal mencari nilai IDF.
Untuk mendapatkan IDF, pertama kita perlu mencari DF (frekuensi Dokumen). </p>
<p><strong>Contoh :</strong></p>
<ul>
<li>Artikel 1 : siswa itu sedang belajar</li>
<li>Artikel 2 : siswa itu sedang bermain bola</li>
</ul>
<p><strong><u>Kata                   Jumlah Kata</u></strong></p>
<p>siswa                           2</p>
<p>itu                              2</p>
<p>sedang                       2</p>
<p>belajar                       1</p>
<p>bermain                     1</p>
<p>bola                                1</p>
<p>=&gt; <strong>Source Code</strong> </p>
<p><code>df = list()
total_doc = bow.shape[0]
for kolom in range(len(bow[0])):
total = 0
for baris in range(len(bow)):
if (bow[baris, kolom] &gt; 0):
total +=1
df.append(total)
df = np.array(df)
idf = list()
for i in df:
tmp = 1 + log10(total_doc/(1+i))
idf.append(tmp)
idf = np.array(idf)
tfidf = bow * idf</code></p>
<p>==========================</p>
<h3 id="preprocessing">PREPROCESSING<a class="headerlink" href="#preprocessing" title="Permanent link">&para;</a></h3>
<p>Tahap preprocessing merupakan tahap mengolah data agar data lebih mudah diproses. Untuk teman-teman tidak diharuskan mengunakaan python saja, namun terserah teman-teman
Tpi pada penejalasan ini saya menggunakan python</p>
<p><strong>Seleksi Fitur</strong> - Seleksi fitur merupakan salah satu cara untuk mengurangi dimensi fitur yang sangat banyak. Seperti pada kasus kita, Text Mining, jumlah fitur yang didapatkan bisa mencapai lebih dari 2000 kata yang berbeda. Namun, tidak semua kata tersebut benar-benar berpengaruh pada hasil akhir nantinya.
Selain itu, kita tahu bahwa semakin banyak data yang diproses, maka lebih banyak biaya dan waktu yang digunakan untuk memprosesnya. Oleh karena itu, kita perlu melakukan pengurangan fitur tanpa mengurangi kualitas hasil akhir, misalnya dengan Seleksi Fitur.
Pada dasarnya, seleksi fitur memiliki 3 tipe umum:</p>
<ol>
<li>Wrap</li>
<li>Filter</li>
<li>Embed</li>
</ol>
<p>=&gt; <strong>Source Code</strong> Seleksi Fitur</p>
<p><code>def seleksiFiturPearson(data, threshold):
global meanFitur
meanFitur = meanF(data)
u=0
while u &lt; len(data[0]):
dataBaru=data[:, :u+1]
meanBaru=meanFitur[:u+1]
v = u
while v &lt; len(data[0]):
if u != v:
value = pearsonCalculate(data, u,v)
if value &lt; threshold:
dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1)))
meanBaru = np.hstack((meanBaru, meanFitur[v]))
v+=1
data = dataBaru
meanFitur=meanBaru
if u%50 == 0 : print(u, data.shape)
u+=1
return data</code></p>
<p>==========================</p>
<p><strong>Pearson Corrlation</strong></p>
<p>Pendekatan Pearson merupakan pendekatan paling sederhana. Pada pendekatan ini, setiap fitur akan dihitung korelasinya. Semakin tinggi nilainya, maka fitur tersebut semakin kuat korelasinya. Lalu fitur yang memiliki korelasi tinggi akan dibuang salah satunya.
Pendekatan ini digunakan untuk data tipe numerik.
Kodenya bisa dilihat dibawah ini  </p>
<p><code>def pearsonCalculate(data, u,v):
"i, j is an index"
atas=0; bawah_kiri=0; bawah_kanan = 0
for k in range(len(data)):
atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v])
bawah_kiri += (data[k,u] - meanFitur[u])**2
bawah_kanan += (data[k,v] - meanFitur[v])**2
bawah_kiri = bawah_kiri ** 0.5
bawah_kanan = bawah_kanan ** 0.5
return atas/(bawah_kiri * bawah_kanan)
def meanF(data):
meanFitur=[]
for i in range(len(data[0])):
meanFitur.append(sum(data[:,i])/len(data))
return np.array(meanFitur)</code></p>
<p>==========================</p>
<p><strong>Clustering</strong></p>
<p>Clustering merupakan pengelompokan data menjadi k-kelompok (dengan k merupakan banyak kelompok). Pengelompkan tersebut berdasarkan ciri yang mirip. Pada kasus ini, maka ciri yang mirip bisa diketahui dari kata yang menjadi ciri dari setiap dokumen.
Metode Clustering sendiri ada banyak. Salah duanya adalah K-Means Clustering dan Fuzzy C-Means Clustering.
Setelah dilakukan proses Clustering, perlu kita cari nilai Silhouette Coefficient untuk melihat apakah hasil cluster tersebut sudah bagus atau tidak.</p>
<p>==========================</p>
<p><strong>K-Means</strong></p>
<p>=&gt; Source Code dibawah ini digunakan untuk melakukan clustering.</p>
<p><code>kmeans = KMeans(n_clusters=5, random_state=0).fit(tfidf_matrix.todense())
for i in range(len(kmeans.labels_)):
print("Doc %d =&gt;&gt; cluster %d" %(i+1, kmeans.labels_[i]))</code></p>
<p>==========================</p>
<p><strong>Klasifikasi</strong></p>
<p>Berbeda dengan Clustering yang mengelompokkan data, klasifikasi lebih mirip pada peramalan data dengan merujuk kategori. Contoh simplenya, seperti peramalan golf. Jika cuaca cerah, angin tidak kencang, maka akan diadakan pertandingan golf. Jika cuaca cerah tapi angin kencang, maka pertandingan golf dibatalkan.
Proses ini biasanya memerlukan data latih dan data test. Untuk menguji keberhasilannya, diperlukan untuk menghitung akurasi atau confution matrix.
Metode-metode yang sering dipakai untuk klasifikasi di antaranya Naive Bayes.</p>
<p>==========================</p>
<p><strong>Naive Bayes</strong></p>
<p>Naive bayes termasuk pada supervised learning berdasarkan teorema Bayes, dengan asumsi "naive", bahwa setiap fitur tidak saling terkait (independen). Terdapat beberapa jenis Naive Bayes, seperti Gaussian NB, Multinomial NB, dll. Namun pada kasus text classification, Multinomial lebih cocok digunakan.
Sebelum itu, kita perlu membagi data menjadi dua bagian: data training, dan data test</p>
<p><code>x_train, x_test, y_train, y_test = train_test_split(xBaru1, label, test_size=0.33, random_state=0)
model = MultinomialNB().fit(x_train, y_train)
predicted = model.predict(x_test)</code></p>
<p>==========================</p>
<h3 id="referensi">REFERENSI<a class="headerlink" href="#referensi" title="Permanent link">&para;</a></h3>
<ul>
<li><a href="">https://github.com/sastrawi/sastrawi/wiki/Stemming-Bahasa-Indonesia</a></li>
<li><a href="https://www.codepolitan.com/stemming-word-dalam-carik-bot-59a9ef6e96088">https://www.codepolitan.com/stemming-word-dalam-carik-bot-59a9ef6e96088</a></li>
<li><a href="https://www.suara.com/bola/bola-category/bola-indonesia">https://www.suara.com/bola/bola-category/bola-indonesia</a></li>
<li><a href="">http://id.dbpedia.org/page/Tokenisasi</a></li>
<li><a href="">https://github.com</a></li>
<li><a href="">https://python.org</a></li>
<li><a href="">https://jupyter.org</a></li>
</ul>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2016 - 2019 Martin Donath
          </div>
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
  <div class="md-footer-social">
    <link rel="stylesheet" href="../assets/fonts/font-awesome.css">
    
      <a href="http://struct.cc" class="md-footer-social__link fa fa-globe"></a>
    
      <a href="https://github.com/squidfunk" class="md-footer-social__link fa fa-github-alt"></a>
    
      <a href="https://twitter.com/squidfunk" class="md-footer-social__link fa fa-twitter"></a>
    
      <a href="https://linkedin.com/in/squidfunk" class="md-footer-social__link fa fa-linkedin"></a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/application.8c0d971c.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:".."}})</script>
      
    
  </body>
</html>